{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 3. Pre-Modeling Steps\n",
                "Before feeding data into an algorithm, you must prepare the environment and refine the inputs. This involves transforming raw data into meaningful signals, filtering out noise, and setting up a rigorous testing environment."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Feature Engineering üèóÔ∏è\n",
                "At its core, feature engineering is about transforming raw data into a format that makes it easier for the machine learning algorithm to understand the underlying patterns.\n",
                "\n",
                "- **Concept**: Brainstorm and create highly predictive new variables from raw data.\n",
                "- **Example**: Subtracting `Year_Built` from `Current_Year` to create a new `Age` feature for predicting house prices. This hands the algorithm the exact mathematical relationship it needs to predict the target."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "# 1. Load the raw data\n",
                "data = {'House_ID': [1, 2, 3], \n",
                "        'Year_Built': [1990, 2005, 2020], \n",
                "        'Current_Year': [2026, 2026, 2026]}\n",
                "df = pd.DataFrame(data)\n",
                "\n",
                "# 2. Engineer the new feature\n",
                "df['House_Age'] = df['Current_Year'] - df['Year_Built']\n",
                "\n",
                "print(df[['House_ID', 'Year_Built', 'House_Age']])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Feature Selection ‚úÇÔ∏è\n",
                "Feeding a model too many irrelevant features introduces noise. The model might accidentally find fake patterns and memorize them (overfitting), which ruins its ability to predict new data. This step removes redundant or irrelevant features to reduce noise and compute time.\n",
                "\n",
                "### Filter Methods (The Bouncer)\n",
                "A fast, statistical check done before training. If a feature has zero mathematical correlation (e.g., via correlation scores or Chi-square) to the target, it gets dropped immediately.\n",
                "\n",
                "### Wrapper Methods (The Tryout)\n",
                "A thorough, iterative process like **Recursive Feature Elimination (RFE)**. It trains a model with all features, drops the least predictive one, and repeats until only the most predictive features remain."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.datasets import make_regression\n",
                "from sklearn.feature_selection import SelectKBest, f_regression, RFE\n",
                "from sklearn.linear_model import LinearRegression\n",
                "\n",
                "# Generate dummy data: 100 samples, 10 features\n",
                "X, y = make_regression(n_samples=100, n_features=10, noise=0.1, random_state=42)\n",
                "\n",
                "# --- Filter Method: SelectKBest ---\n",
                "# Keep only the top 3 features most strongly correlated with the target\n",
                "selector = SelectKBest(score_func=f_regression, k=3)\n",
                "X_filtered = selector.fit_transform(X, y)\n",
                "\n",
                "# --- Wrapper Method: Recursive Feature Elimination (RFE) ---\n",
                "model = LinearRegression()\n",
                "rfe = RFE(estimator=model, n_features_to_select=3)\n",
                "X_rfe = rfe.fit_transform(X, y)\n",
                "\n",
                "print(f\"Selected feature indices (RFE): {rfe.support_}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Dimensionality Reduction (PCA)\n",
                "Sometimes features are highly related to each other (e.g., \"Total Square Footage\" and \"Number of Rooms\" in a house). Instead of dropping them, we compress them.\n",
                "\n",
                "- **Principal Component Analysis (PCA)**: Mathematically mashes correlated features together into new, consolidated variables called \"Principal Components.\" You lose human readability, but you keep the underlying variance (the important information) while drastically shrinking the dataset size."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.decomposition import PCA\n",
                "\n",
                "# Compress the 10 original features down to 2 principal components\n",
                "pca = PCA(n_components=2)\n",
                "X_pca = pca.fit_transform(X)\n",
                "\n",
                "print(f\"PCA reduced shape: {X_pca.shape}\")\n",
                "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Data Splitting & K-Fold Cross-Validation\n",
                "Once features are engineered and selected, you cannot feed 100% of your data into the algorithm. You must rigorously test it to prevent overfitting (memorizing the data instead of learning patterns).\n",
                "\n",
                "### Train, Validation, and Test Split\n",
                "- **Training Set (~70-80%)**: The \"textbook\". The algorithm uses this data to learn the relationships and mathematical weights.\n",
                "- **Validation Set (~10-15%)**: The \"practice quiz\". Used to evaluate the model while you are still tweaking its settings (hyperparameter tuning). \n",
                "- **Test Set (~10-15%)**: The \"final exam\". Locked in a vault until the end. Gives you the final, unbiased metric of how the model performs in production.\n",
                "\n",
                "### K-Fold Cross-Validation\n",
                "A simple Train/Test split might be \"lucky\" or \"unlucky\". K-Fold Cross-Validation solves this by dividing data into \"K\" equal-sized chunks (e.g., K=5):\n",
                "- The model trains on 4 folds, validates on the 1st fold.\n",
                "- It resets, trains on a different combination of 4 folds, and validates on the 2nd fold.\n",
                "- This repeats 5 times, so every data point is in the validation set exactly once.\n",
                "- Averaging the 5 scores proves the model is stable and performs consistently."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
                "\n",
                "# --- 1. Train/Test Split ---\n",
                "# Split data into 80% training and 20% testing\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "# --- 2. K-Fold Cross-Validation ---\n",
                "# Divide the training data into 5 distinct \"folds\"\n",
                "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
                "\n",
                "# Train and evaluate the model 5 times (each time using a different fold for validation)\n",
                "cv_scores = cross_val_score(model, X_train, y_train, cv=kf)\n",
                "\n",
                "print(f\"Individual Fold Scores: {cv_scores}\")\n",
                "print(f\"Average Model Performance: {cv_scores.mean():.4f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}