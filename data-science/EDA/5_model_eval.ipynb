{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Step 5: Model Evaluation ðŸŽ¯\n",
                "\n",
                "How we grade the model depends entirely on the business problem. For ZS interviews, focus on translating these metrics into business impact."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Classification Metrics: The Confusion Matrix\n",
                "\n",
                "| | **Predicted: Yes (Positive)** | **Predicted: No (Negative)** |\n",
                "| :--- | :--- | :--- |\n",
                "| **Actual: Yes** | **True Positive (TP)**: Correct Hit | **False Negative (FN)**: Missed Opportunity (Type II Error) |\n",
                "| **Actual: No** | **False Positive (FP)**: False Alarm (Type I Error) | **True Negative (TN)**: Correct Rejection |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Essential Formulas\n",
                "\n",
                "- **Accuracy**: Percentage of total predictions correct.\n",
                "  $$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
                "\n",
                "- **Precision**: Efficiency of marketing spend. \"Out of all 'Yes' predictions, how many were right?\"\n",
                "  $$Precision = \\frac{TP}{TP + FP}$$\n",
                "\n",
                "- **Recall (Sensitivity)**: Market Share capture. \"Out of all actual 'Yes' cases, how many did we find?\"\n",
                "  $$Recall = \\frac{TP}{TP + FN}$$\n",
                "\n",
                "- **F1-Score**: Harmonic mean of Precision and Recall. Great for imbalanced datasets.\n",
                "  $$F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$$\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ROC-AUC Deep Dive\n",
                "\n",
                "- **ROC Full Form**: **Receiver Operating Characteristic**\n",
                "- **Meaning**: The ROC curve is a geometric plot that visualizes a model's performance across all possible classification thresholds. It plots the relationship between how many \"True Positives\" we catch vs. how many \"False Alarms\" we trigger.\n",
                "\n",
                "#### The Axis Formulas:\n",
                "1. **True Positive Rate (TPR)**: Also known as **Recall** or **Sensitivity**. It measures the proportion of actual positives correctly identified.\n",
                "   $$TPR = \\frac{TP}{TP + FN}$$\n",
                "\n",
                "2. **False Positive Rate (FPR)**: It measures the proportion of actual negatives that were incorrectly classified as positives.\n",
                "   $$FPR = \\frac{FP}{FP + TN}$$\n",
                "\n",
                "**AUC (Area Under the Curve)**: A single number from 0 to 1 representing the total area under the ROC curve. \n",
                "- **AUC = 0.5**: Random guessing.\n",
                "- **AUC = 1.0**: Perfect separation.\n",
                "- **ZS Tip**: AUC > 0.8 is generally considered good for real-world business cases."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Regression Metrics: Predicting Numbers\n",
                "\n",
                "- **Mean Absolute Error (MAE)**: Average error in real-world units (e.g., \"Off by $10,000 on average\").\n",
                "- **Root Mean Squared Error (RMSE)**: Penalizes large errors heavily.\n",
                "- **R-Squared ($R^2$)**: Percentage of variance explained by the model."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. ZS Case Study Context: Which Metric Matters?\n",
                "\n",
                "- **Expensive Campaign**: High **Precision** is key (don't waste money sending sales reps to wrong doctors).\n",
                "- **Rare Disease/High Stakes**: High **Recall** is key (missing one patient could be critical).\n",
                "- **Imbalanced Classes**: Ignore Accuracy; use **F1-Score** or **ROC-AUC**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
                "\n",
                "# Assuming y_test and y_pred are defined\n",
                "# print(confusion_matrix(y_test, y_pred))\n",
                "# print(classification_report(y_test, y_pred))\n",
                "# print(f\"ROC-AUC: {roc_auc_score(y_test, y_prob):.4f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}