{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eae409a",
   "metadata": {},
   "source": [
    "# üéØ Interview-Ready Project Explanation\n",
    "\n",
    "---\n",
    "\n",
    "## NL-to-SQL Agentic Pipeline Optimization\n",
    "\n",
    "### üó£Ô∏è How to Explain It (STAR Format)\n",
    "\n",
    "| Component | What to Say |\n",
    "|---|---|\n",
    "| **Situation** | \"I was working on an agentic NL-to-SQL pipeline where users ‚Äî primarily managers ‚Äî could query databases using plain English. The system was functional but slow and expensive.\" |\n",
    "| **Task** | \"My job was to reduce latency and LLM token consumption without compromising query accuracy.\" |\n",
    "| **Action** | *(see breakdown below)* |\n",
    "| **Result** | \"Response time dropped by **50%** and token usage came down from **80% to 40%** over time.\" |\n",
    "\n",
    "---\n",
    "\n",
    "### üî¨ The Action ‚Äî Say It Like This:\n",
    "\n",
    "> *\"Before jumping to solutions, I did a data-driven analysis of the query logs. I noticed that queries clustering in the same semantic vector space were often identical in **intent** but only differed in **time references** ‚Äî like a manager asking 'sales last month' vs 'sales last quarter'. This happened in roughly **70% of cases**.\"*\n",
    "\n",
    "> *\"So I built a caching layer on top of the pipeline. I used **spaCy's NER model** to detect and extract temporal phrases from incoming queries. Instead of generating a full SQL every time, we generated **Placeholder SQLs** ‚Äî parameterized templates ‚Äî and stored them in the Vector DB.\"*\n",
    "\n",
    "> *\"On subsequent similar queries, we'd fetch the cached template and **only generate the dynamic date tokens**, skipping full LLM inference entirely. Think of it as surgical token generation instead of full regeneration.\"*\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö° One-Liner for Impression\n",
    "\n",
    "> *\"Essentially I turned a repetitive full-inference problem into a **semantic cache + partial generation** problem ‚Äî and let the data tell me where to optimize.\"*\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ Follow-Up Questions You Should Prep For\n",
    "\n",
    "| Question | Quick Answer |\n",
    "|---|---|\n",
    "| *How did you measure semantic similarity?* | Cosine similarity on vector embeddings in the VectorDB |\n",
    "| *What if spaCy misses a date phrase?* | Fallback to full LLM generation ‚Äî cache miss handled gracefully |\n",
    "| *Why not just use exact query caching?* | Natural language is never identical ‚Äî semantic matching was essential |\n",
    "| *How did token usage drop over time?* | As cache hit rate grew, fewer full LLM calls were made ‚Äî compounding savings |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
